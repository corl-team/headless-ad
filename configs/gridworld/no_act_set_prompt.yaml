
# wandb
project: Headless-AD
group: gridworld_headless_no_act_pool_optimal
job_type: debug
name: null

# Tuned
use_action_set_prompt: False

attention_dropout: 0.4
beta1: 0.99
dropout: 0.16
learning_rate: 3.5e-4
tau: 8.5
weight_decay: 4.7e-3
num_in_context_episodes: 1000
action_space_type: for_headless

# seeds
train_seed: 0
eval_seed: 100

# data settings
env_name: GridWorld
num_train_envs: 10_000
learning_histories_path: trajectories

num_eval_envs: 100
eval_every: 1_000
log_every: 100

num_train_steps: 30_000

# Model Params
seq_len: 100
layer_norm_bias: True
token_embed_dim: 128
d_model: 512
num_layers: 4
num_heads: 64

# Training params
batch_size: 64
warmup_ratio: 0.1
clip_grad_norm: 5
sim_measure: dot
get_action_type: sample
rand_select_in_ucb: True
loss_type: contrastive

# New
rotary_percentage: 1.0  # is default in the llama's configs at the bottom
parallel_residual: False
shared_attention_norm: False
_norm_class: FusedRMSNorm
_mlp_class: LLaMAMLP

# Device
device: cuda
autocast_dtype: bf16

# Where to save data for experiment visualizations
logs_dir: logs

# New
action_seq_len: 3
train_frac_acts: 0.4
train_frac_goals: 0.85
grid_size: 9
num_episodes: 200
q_learning_lr: 0.9933
q_learning_discount: 0.6238
check_cached: False
