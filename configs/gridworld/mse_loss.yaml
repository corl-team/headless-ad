# wandb
project: Headless-AD
group: gridworld_headless_mse_optimal
job_type: debug
name: null

# Tuned
attention_dropout: 0.08
beta1: 0.75
dropout: 0.75
learning_rate: 1.3e-5
weight_decay: 2.5
num_in_context_episodes: 1000
action_space_type: for_headless
loss_type: mse
get_action_type: mode

# seeds
train_seed: 0
eval_seed: 100

# data settings
env_name: GridWorld
num_train_envs: 10_000
learning_histories_path: trajectories

num_eval_envs: 100
eval_every: 1_000
log_every: 100

num_train_steps: 30_000

# Model Params
seq_len: 100
layer_norm_bias: True
token_embed_dim: 128
d_model: 512
num_layers: 4
num_heads: 64

# Training params
batch_size: 64
warmup_ratio: 0.1
clip_grad_norm: 5
sim_measure: dot
rand_select_in_ucb: True

# New
rotary_percentage: 1.0  # is default in the llama's configs at the bottom
parallel_residual: False
shared_attention_norm: False
_norm_class: FusedRMSNorm
_mlp_class: LLaMAMLP

# Device
device: cuda
autocast_dtype: bf16

# Where to save data for experiment visualizations
logs_dir: logs

# New
action_seq_len: 3
train_frac_acts: 0.4
train_frac_goals: 0.85
grid_size: 9
num_episodes: 200
q_learning_lr: 0.9933
q_learning_discount: 0.6238
check_cached: False
use_action_set_prompt: True